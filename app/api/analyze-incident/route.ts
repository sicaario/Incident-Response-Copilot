import { type NextRequest, NextResponse } from "next/server"

// Use Edge Runtime for better performance and reliability
export const runtime = "edge"

// Get our machine learning API keys from environment
const GROQ_API_KEY = process.env.GROQ_API_KEY
const TAVILY_API_KEY = process.env.TAVILY_API_KEY
const MEM0_API_KEY = process.env.MEM0_API_KEY

interface AnalysisRequest {
  incidentId: string
  logContent: string
  fileId?: string
}

// Simple health check endpoint
export async function GET() {
  console.log("üîç Analysis API health check")

  return NextResponse.json({
    message: "Analysis API is running on Edge Runtime",
    environment: {
      hasGroqKey: !!GROQ_API_KEY,
      hasTavilyKey: !!TAVILY_API_KEY,
      hasMem0Key: !!MEM0_API_KEY,
      runtime: "edge",
    },
    timestamp: new Date().toISOString(),
  })
}

// Main analysis endpoint - this is where the magic happens
export async function POST(request: NextRequest) {
  console.log("üöÄ Starting incident analysis")

  try {
    // First, make sure we have all our API keys
    const missingKeys = []
    if (!GROQ_API_KEY) missingKeys.push("GROQ_API_KEY")
    if (!TAVILY_API_KEY) missingKeys.push("TAVILY_API_KEY")
    if (!MEM0_API_KEY) missingKeys.push("MEM0_API_KEY")

    if (missingKeys.length > 0) {
      console.error("‚ùå Missing API keys:", missingKeys)
      return NextResponse.json(
        {
          error: "Server configuration error: Missing API keys",
          missingKeys,
        },
        { status: 500 },
      )
    }

    console.log("‚úÖ All machine learning API keys present")

    // Parse the request data
    const body = await request.json()
    const { incidentId, logContent }: AnalysisRequest = body

    console.log("üìù Request data:", {
      incidentId: incidentId?.slice(0, 8),
      logContentLength: logContent?.length,
      hasIncidentId: !!incidentId,
      hasLogContent: !!logContent,
    })

    // Validate we have the required data
    if (!incidentId || !logContent) {
      console.error("‚ùå Missing required fields:", { hasIncidentId: !!incidentId, hasLogContent: !!logContent })
      return NextResponse.json({ error: "Missing required fields: incidentId and logContent" }, { status: 400 })
    }

    console.log(`‚úÖ Starting analysis for incident: ${incidentId}`)

    // Run our machine learning pipeline
    console.log("üöÄ Running machine learning pipeline...")
    const analysisResults = await runMachineLearningPipeline(logContent)

    return NextResponse.json({
      success: true,
      message: "Analysis completed successfully",
      results: analysisResults,
      incidentId,
    })
  } catch (error: any) {
    console.error("‚ùå Analysis API error:", {
      message: error.message,
      stack: error.stack,
      name: error.name,
    })
    return NextResponse.json(
      { error: `Analysis failed: ${error instanceof Error ? error.message : "Unknown error"}` },
      { status: 500 },
    )
  }
}

// Our main machine learning pipeline - this coordinates all the agents
async function runMachineLearningPipeline(logContent: string) {
  try {
    console.log(`üöÄ Starting machine learning pipeline`)

    // Step 1: Parse the logs to extract errors and important info
    console.log("üîç Step 1: Running log parsing agent...")
    const parsedData = await logParsingAgent(logContent)
    console.log("‚úÖ Log parsing completed:", {
      errorCount: parsedData.errors.length,
    })

    // Step 2: Analyze the root cause of the issues
    console.log("üß† Step 2: Running root cause analysis agent...")
    const rootCause = await rootCauseAgent(logContent, parsedData.errors)
    console.log("‚úÖ Root cause analysis completed")

    // Step 3: Search the web for similar issues and solutions
    console.log("üîé Step 3: Running research agent...")
    const externalContext = await researchAgent(parsedData.errors, rootCause)
    console.log("‚úÖ Research completed")

    // Step 4: Generate actionable solutions
    console.log("üí° Step 4: Running solution synthesis agent...")
    const solutions = await solutionAgent(logContent, rootCause, externalContext)
    console.log("‚úÖ Solution generation completed")

    console.log("üéâ Machine learning pipeline completed successfully")

    return {
      parsed_errors: parsedData.errors || [],
      root_cause: rootCause || "Unable to determine root cause",
      external_context: externalContext || "No external research available",
      recommended_solutions: solutions || "No solutions generated",
      resolution_status: "resolved", 
    }
  } catch (error: any) {
    console.error("‚ùå Machine learning pipeline error:", error)
    throw error
  }
}

// Agent 1: Parse logs and extract errors
async function logParsingAgent(logContent: string) {
  try {
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "llama3-70b-8192",
        messages: [
          {
            role: "system",
            content: `Extract errors and critical issues from log content. Return JSON with 'errors' array.`,
          },
          {
            role: "user",
            content: `Parse this log content:\n\n${logContent.slice(0, 4000)}`,
          },
        ],
        temperature: 0.1,
        max_tokens: 1000,
      }),
    })

    if (!response.ok) {
      throw new Error(`Groq API error: ${response.status}`)
    }

    const data = await response.json()
    const content = data.choices[0].message.content

    try {
      // Try to parse as JSON first
      const parsed = JSON.parse(content)
      return {
        errors: Array.isArray(parsed.errors) ? parsed.errors.slice(0, 10) : [],
      }
    } catch {
      // If JSON parsing fails, extract error lines manually
      const lines = content
        .split("\n")
        .filter((line) => line.toLowerCase().includes("error"))
        .slice(0, 5)
      return {
        errors: lines.length > 0 ? lines : ["Log parsing completed - no specific errors identified"],
      }
    }
  } catch (error: any) {
    return {
      errors: [`Log parsing failed: ${error.message}`],
    }
  }
}

// Agent 2: Analyze root cause of the issues
async function rootCauseAgent(logContent: string, errors: string[]) {
  try {
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "llama3-70b-8192",
        messages: [
          {
            role: "system",
            content: `Analyze log content and identify the root cause. Be specific and technical.`,
          },
          {
            role: "user",
            content: `Log: ${logContent.slice(0, 2000)}\nErrors: ${errors.slice(0, 5).join("\n")}`,
          },
        ],
        temperature: 0.2,
        max_tokens: 400,
      }),
    })

    if (!response.ok) {
      throw new Error(`Groq API error: ${response.status}`)
    }

    const data = await response.json()
    return data.choices[0].message.content.slice(0, 800)
  } catch (error: any) {
    return `Root cause analysis failed: ${error.message}`
  }
}

// Agent 3: Research similar issues online
async function researchAgent(errors: string[], rootCause: string) {
  try {
    // Extract and score keywords from errors
    const keywordWeights = new Map<string, number>();
    
    errors.concat(rootCause).forEach(text => {
      // Split while preserving technical constructs
      const tokens = text.match(/([A-Z]{2,}(?=[A-Z]|\b)|[\w#]+|\d{3,})/g) || [];
      
      tokens.forEach(token => {
        const normalized = token.toLowerCase();
        const isErrorCode = /^[a-z]+\d+$|^\d{3,}$/i.test(token);
        const isTechTerm = /(error|exception|fail|timeout|undefined|null)/i.test(token);
        const isRareWord = token.length > 8;
        
        let weight = 1;
        if (isErrorCode) weight = 3;
        else if (isTechTerm) weight = 2;
        else if (isRareWord) weight = 1.5;
        
        keywordWeights.set(normalized, 
          (keywordWeights.get(normalized) || 0) + weight
        );
      });
    });

    // Remove stopwords and sort by relevance
    const stopWords = new Set(['the', 'and', 'at', 'in', 'of', 'to', 'a']);
    const sortedKeywords = Array.from(keywordWeights.entries())
      .filter(([word]) => !stopWords.has(word) && word.length > 2)
      .sort((a, b) => b[1] - a[1])
      .slice(0, 5)
      .map(([word]) => word);

    // Build StackOverflow-optimized query
    const queryParts = [
      ...sortedKeywords,
      ...rootCause.split(/\W+/).filter(w => w.length > 3),
      "stackoverflow"
    ];
    
    const searchQuery = Array.from(new Set(queryParts))
      .join(" ")
      .substring(0, 100);

    if (!searchQuery) return "No valid search terms extracted";

    // Search with StackOverflow/Github focus
    const response = await fetch("https://api.tavily.com/search", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        api_key: TAVILY_API_KEY,
        query: searchQuery,
        search_depth: "advanced",  // Deeper analysis
        include_domains: ["stackoverflow.com", "github.com"],
        max_results: 5,            // More results
        include_answer: true        // Include summarized answers
      })
    });

    if (!response.ok) throw new Error(`Tavily API error: ${response.status}`);

    const data = await response.json();
    if (!data.results?.length) return "No relevant solutions found";

    // Prioritize StackOverflow results
    return data.results
      .sort((a, b) => 
        b.url.includes("stackoverflow") - a.url.includes("stackoverflow")
      )
      .slice(0, 3) // Top 3 most relevant
      .map(result => {
        const source = result.url.includes("stackoverflow")
          ? "üî• StackOverflow" 
          : "üêô GitHub";
          
        return `${source} | ${result.title}\n${result.content.split("\n")[0]}\n${result.url}`;
      })
      .join("\n\n");
      
  } catch (error) {
    return `Research failed: ${error.message}`;
  }
}

// Agent 4: Generate actionable solutions
async function solutionAgent(logContent: string, rootCause: string, externalContext: string) {
  try {
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${GROQ_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "llama3-70b-8192",
        messages: [
          {
            role: "system",
            content: `Generate specific, actionable solutions for incident resolution. Provide step-by-step instructions.`,
          },
          {
            role: "user",
            content: `Root Cause: ${rootCause}\n\nContext: ${externalContext.slice(0, 800)}\n\nProvide solutions:`,
          },
        ],
        temperature: 0.3,
        max_tokens: 600,
      }),
    })

    if (!response.ok) {
      throw new Error(`Groq API error: ${response.status}`)
    }

    const data = await response.json()
    return data.choices[0].message.content.slice(0, 1500)
  } catch (error: any) {
    return `Solution generation failed: ${error.message}`
  }
}
